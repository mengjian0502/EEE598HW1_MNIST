Namespace(batch_size=128, depth=[784, 400, 400], drop_rate=0.5, epochs=30, evaluate=False, gammas=[0.1, 0.1], log_file='cnn_mnist_lr0.1_wd1e-4.log', lr=0.1, model='cnn_mnist', momentum=0.9, resume='', save_path='./save/cnn_mnist/cnn_mnist_lr0.1_wd1e-4_p0.5/', schedule=[20, 25], use_cuda=True, weight_decay=0.0001)
==> Building model..

Net(
  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))
  (relu1): ReLU(inplace=True)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))
  (relu2): ReLU(inplace=True)
  (dropout1): Dropout(p=0.5, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): Linear(in_features=4608, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=10, bias=True)
)
----  --------  ---------  --------  ---------  --------  ----------
  ep        lr    tr_loss    tr_acc    te_loss    te_acc    best_acc
----  --------  ---------  --------  ---------  --------  ----------
   1    0.1000     0.3758   88.2717     0.0798   97.6300     97.6300
   2    0.1000     0.2036   94.0033     0.0677   98.0000     98.0000
   3    0.1000     0.1752   94.9267     0.0921   97.2800     98.0000
   4    0.1000     0.1736   94.9567     0.0563   98.3500     98.3500
   5    0.1000     0.1490   95.6317     0.0496   98.5000     98.5000
   6    0.1000     0.1494   95.6083     0.0550   98.2900     98.5000
   7    0.1000     0.1463   95.7733     0.0570   98.4200     98.5000
   8    0.1000     0.1348   96.0500     0.0402   98.8400     98.8400
   9    0.1000     0.1307   96.2000     0.0604   98.3100     98.8400
  10    0.1000     0.1347   96.1067     0.0444   98.6600     98.8400
  11    0.1000     0.1289   96.2883     0.0376   98.7000     98.8400
  12    0.1000     0.1251   96.3233     0.0496   98.3900     98.8400
  13    0.1000     0.1149   96.7100     0.0425   98.6700     98.8400
  14    0.1000     0.1180   96.5600     0.0399   98.8100     98.8400
  15    0.1000     0.1128   96.7533     0.0471   98.4700     98.8400
  16    0.1000     0.1135   96.6883     0.0452   98.7300     98.8400
  17    0.1000     0.1125   96.7267     0.0471   98.5000     98.8400
  18    0.1000     0.1112   96.8117     0.0410   98.8000     98.8400
  19    0.1000     0.1052   96.9550     0.0439   98.6700     98.8400
  20    0.1000     0.1068   96.8900     0.0445   98.6400     98.8400
  21    0.0100     0.0683   97.8650     0.0325   99.0700     99.0700
  22    0.0100     0.0574   98.2233     0.0322   99.0100     99.0700
  23    0.0100     0.0564   98.2967     0.0308   99.1000     99.1000
  24    0.0100     0.0509   98.5000     0.0295   99.1000     99.1000
  25    0.0100     0.0508   98.4850     0.0286   99.1200     99.1200
  26    0.0010     0.0479   98.5800     0.0290   99.1600     99.1600
  27    0.0010     0.0473   98.5733     0.0293   99.1200     99.1600
  28    0.0010     0.0461   98.6167     0.0294   99.1500     99.1600
  29    0.0010     0.0460   98.6217     0.0293   99.1200     99.1600
  30    0.0010     0.0440   98.5967     0.0293   99.1100     99.1600
Namespace(batch_size=128, depth=[784, 400, 400], drop_rate=0.5, epochs=30, evaluate=False, gammas=[0.1, 0.1], log_file='cnn_mnist_lr0.1_wd1e-4.log', lr=0.1, model='cnn_mnist', momentum=0.9, resume='', save_path='./save/cnn_mnist/cnn_mnist_lr0.1_wd1e-4_p0.5/', schedule=[20, 25], use_cuda=True, weight_decay=0.0001)
==> Building model..

Net(
  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))
  (relu1): ReLU(inplace=True)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))
  (relu2): ReLU(inplace=True)
  (dropout1): Dropout(p=0.5, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): Linear(in_features=4608, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=10, bias=True)
)
----  --------  ---------  --------  ----------  ---------  ----------
  ep        lr    tr_loss    tr_acc    val_loss    val_acc    best_acc
----  --------  ---------  --------  ----------  ---------  ----------
   1    0.1000     0.4315   86.4420      0.0829    97.6500     97.6500
   2    0.1000     0.1852   94.4260      0.0676    98.0500     98.0500
   3    0.1000     0.1505   95.5140      0.0520    98.5000     98.5000
   4    0.1000     0.1302   96.0820      0.0597    98.3800     98.5000
   5    0.1000     0.1255   96.2820      0.0465    98.7500     98.7500
   6    0.1000     0.1258   96.2780      0.0516    98.4900     98.7500
   7    0.1000     0.1111   96.6220      0.0517    98.5700     98.7500
   8    0.1000     0.1056   96.7380      0.0455    98.7100     98.7500
   9    0.1000     0.1001   96.9900      0.0486    98.5100     98.7500
  10    0.1000     0.1006   97.0320      0.0402    98.9100     98.9100
  11    0.1000     0.0947   97.1520      0.0421    98.8700     98.9100
  12    0.1000     0.0930   97.1840      0.0422    98.8300     98.9100
  13    0.1000     0.0883   97.2720      0.0456    98.9100     98.9100
  14    0.1000     0.0856   97.3740      0.0458    98.8700     98.9100
  15    0.1000     0.0910   97.1900      0.0405    98.9700     98.9700
  16    0.1000     0.0850   97.4440      0.0397    98.9400     98.9700
  17    0.1000     0.0849   97.4280      0.0402    98.9600     98.9700
  18    0.1000     0.0826   97.5200      0.0374    98.9300     98.9700
  19    0.1000     0.0837   97.4860      0.0430    98.8900     98.9700
  20    0.1000     0.0798   97.5880      0.0425    98.8800     98.9700
  21    0.0100     0.0583   98.2120      0.0343    99.1500     99.1500
  22    0.0100     0.0491   98.5040      0.0334    99.1700     99.1700
  23    0.0100     0.0464   98.5780      0.0322    99.1500     99.1700
  24    0.0100     0.0432   98.7160      0.0317    99.2300     99.2300
  25    0.0100     0.0437   98.6700      0.0309    99.2700     99.2700
  26    0.0010     0.0409   98.7480      0.0308    99.2500     99.2700
  27    0.0010     0.0385   98.7900      0.0307    99.2600     99.2700
  28    0.0010     0.0379   98.7880      0.0308    99.2600     99.2700
  29    0.0010     0.0392   98.7340      0.0308    99.2700     99.2700
  30    0.0010     0.0400   98.7200      0.0307    99.2200     99.2700
Test accuracy: 99.2
Namespace(batch_size=128, depth=[784, 400, 400], drop_rate=0.5, epochs=30, evaluate=False, gammas=[0.1, 0.1], log_file='cnn_mnist_lr0.1_wd1e-4.log', lr=0.1, model='cnn_mnist', momentum=0.9, resume='', save_path='./save/cnn_mnist/cnn_mnist_lr0.1_wd1e-4_p0.5/', schedule=[20, 25], use_cuda=True, weight_decay=0.0001)
==> Building model..

Net(
  (conv1): Conv2d(1, 132, kernel_size=(3, 3), stride=(1, 1))
  (relu1): ReLU(inplace=True)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))
  (relu2): ReLU(inplace=True)
  (dropout1): Dropout(p=0.5, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): Linear(in_features=9216, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=10, bias=True)
)
Namespace(batch_size=128, depth=[784, 400, 400], drop_rate=0.5, epochs=30, evaluate=False, gammas=[0.1, 0.1], log_file='cnn_mnist_lr0.1_wd1e-4.log', lr=0.1, model='cnn_mnist', momentum=0.9, resume='', save_path='./save/cnn_mnist/cnn_mnist_lr0.1_wd1e-4_p0.5/', schedule=[20, 25], use_cuda=True, weight_decay=0.0001)
==> Building model..

Net(
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))
  (relu1): ReLU(inplace=True)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))
  (relu2): ReLU(inplace=True)
  (dropout1): Dropout(p=0.5, inplace=False)
  (dropout2): Dropout(p=0.5, inplace=False)
  (fc1): Linear(in_features=9216, out_features=128, bias=True)
  (fc2): Linear(in_features=128, out_features=10, bias=True)
)
----  --------  ---------  --------  ----------  ---------  ----------
  ep        lr    tr_loss    tr_acc    val_loss    val_acc    best_acc
----  --------  ---------  --------  ----------  ---------  ----------
   1    0.1000     0.3856   88.0520      0.0808    97.7700     97.7700
   2    0.1000     0.1708   94.9080      0.0732    97.9200     97.9200
   3    0.1000     0.1340   96.0600      0.0548    98.4900     98.4900
   4    0.1000     0.1156   96.4260      0.0562    98.3500     98.4900
   5    0.1000     0.1022   97.0020      0.0517    98.5000     98.5000
   6    0.1000     0.0931   97.2260      0.0500    98.6000     98.6000
   7    0.1000     0.0925   97.1940      0.0441    98.8400     98.8400
   8    0.1000     0.0810   97.4780      0.0487    98.7900     98.8400
   9    0.1000     0.0763   97.6260      0.0421    98.9000     98.9000
  10    0.1000     0.0708   97.7980      0.0431    98.8400     98.9000
  11    0.1000     0.0722   97.8240      0.0538    98.7200     98.9000
  12    0.1000     0.0662   98.0420      0.0414    99.0200     99.0200
  13    0.1000     0.0594   98.1680      0.0430    98.8600     99.0200
  14    0.1000     0.0602   98.1460      0.0404    99.0100     99.0200
  15    0.1000     0.0595   98.2100      0.0449    98.9500     99.0200
  16    0.1000     0.0565   98.2180      0.0416    98.8800     99.0200
  17    0.1000     0.0575   98.2180      0.0397    99.0200     99.0200
  18    0.1000     0.0550   98.2600      0.0405    98.8700     99.0200
  19    0.1000     0.0542   98.3460      0.0372    98.9800     99.0200
  20    0.1000     0.0561   98.2260      0.0414    98.9500     99.0200
  21    0.0100     0.0352   98.8760      0.0349    99.2200     99.2200
  22    0.0100     0.0298   99.0820      0.0343    99.2100     99.2200
  23    0.0100     0.0253   99.1720      0.0348    99.2500     99.2500
  24    0.0100     0.0259   99.2020      0.0354    99.2100     99.2500
  25    0.0100     0.0242   99.2040      0.0352    99.2300     99.2500
  26    0.0010     0.0232   99.2280      0.0349    99.2200     99.2500
  27    0.0010     0.0222   99.3040      0.0346    99.2400     99.2500
  28    0.0010     0.0224   99.2800      0.0346    99.2400     99.2500
  29    0.0010     0.0211   99.3080      0.0346    99.2400     99.2500
  30    0.0010     0.0208   99.3220      0.0345    99.2400     99.2500
Test accuracy: 99.31
